{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import catboost as ctb\n",
    "import lightgbm as lgb\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"/usr/input/flipr-hackathon-dataset/Train_dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"Name\", \"Designation\", \"Region\", \"people_ID\"],axis = 1)\n",
    "data = data.loc[~data.iloc[:,[0,1,3,4,7,10,11,]].isnull().any(axis=1)]\n",
    "y = data[\"Infect_Prob\"]\n",
    "data = data.drop([\"Infect_Prob\"], axis = 1)\n",
    "#y = (y >= 50).astype(\"float64\")\n",
    "y /= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(data.values, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [0,1,3,4,7,10,11,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters of catboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7fee668ecc50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_cat = ctb.CatBoostRegressor(boosting_type=\"Plain\", loss_function=\"RMSE\", learning_rate=0.01, border_count=254,\\\n",
    "                            verbose=0, n_estimators=10000,\\\n",
    "                            cat_features = cat_features, \\\n",
    "                            depth = 10, l2_leaf_reg = 1, subsample = 0.6,)\n",
    "clf_cat.fit(X_train_cat, y_train_cat, eval_set = [(X_test_cat, y_test_cat)], early_stopping_rounds = 20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006410789035192972"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test_cat, clf_cat.predict(X_test_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters of LiteGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "le_list = []\n",
    "for i in cat_features:\n",
    "    le = LabelEncoder()\n",
    "    data.iloc[:,i] = le.fit_transform(data.iloc[:,i])\n",
    "    le_list.append(copy.copy(le))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.values, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1295: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [0, 1, 2]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.8, bagging_freq=1, boosting_type='gbdt',\n",
       "              class_weight=None, colsample_bytree=0.9, depth=2,\n",
       "              importance_type='split', learning_rate=0.001, max_bin=100000,\n",
       "              max_depth=-1, metric='mse', min_child_samples=20,\n",
       "              min_child_weight=250, min_split_gain=0.0, n_estimators=99999,\n",
       "              n_jobs=-1, n_threads=3, num_leaves=5, objective='regression',\n",
       "              random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
       "              scale_pos_weight=1, silent=True, subsample=1.0,\n",
       "              subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lit = lgb.LGBMRegressor(boosting_type=\"gbdt\",objective=\"regression\",learning_rate=0.001, \\\n",
    "                        bagging_freq=1, \\\n",
    "                        max_bin=100000, scale_pos_weight=1, \\\n",
    "                        metric= \"mse\", n_threads = 3,n_estimators= 99999, \\\n",
    "                        depth = 2, num_leaves = 5, min_child_weight = 250, \\\n",
    "                        colsample_bytree = 0.9, bagging_fraction = 0.8)\n",
    "clf_lit.fit(X_train, y_train, eval_set = [(X_test, y_test)], early_stopping_rounds = 30, verbose = 0, categorical_feature = [0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006359819464519343"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, clf_lit.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006355923461151434"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, (clf_lit.predict(X_test) + clf_cat.predict(X_test_cat)) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"/kaggle/input/flipr-hackathon-dataset/Train_dataset.xlsx\")\n",
    "data = data.drop([\"Name\", \"Designation\", \"Region\", \"people_ID\"],axis = 1)\n",
    "data = data.loc[~data.iloc[:,[0,1,3,4,7,10,11,]].isnull().any(axis=1)]\n",
    "y = data[\"Infect_Prob\"]\n",
    "data = data.drop([\"Infect_Prob\"], axis = 1)\n",
    "#y = (y >= 50).astype(\"float64\")\n",
    "y /= 100\n",
    "cat_features = [0,1,3,4,7,10,11,]\n",
    "data_one_hot = pd.get_dummies(data = data, columns = data.columns[cat_features])\n",
    "imputed_data = data_one_hot.fillna(data.mean())\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(imputed_data.values, y, random_state = 42)\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_nn = scaler.fit_transform(X_train_nn)\n",
    "X_test_nn = scaler.transform(X_test_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7248, 44)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(8, activation='relu', input_dim=44, ), \\\n",
    "                          layers.Dropout(0.2), \\\n",
    "                          layers.Dense(4, activation='relu', ), \\\n",
    "                          layers.Dropout(0.2), \\\n",
    "                          layers.Dense(1, activation = \"linear\"), \\\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"mean_squared_error\", metrics = [\"mse\"])\n",
    "# use early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 10)\n",
    "# save the best model\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_mse', mode='min', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7248 samples, validate on 2417 samples\n",
      "Epoch 1/1000\n",
      "6432/7248 [=========================>....] - ETA: 0s - loss: 0.2162 - mse: 0.2162\n",
      "Epoch 00001: val_mse improved from inf to 0.05945, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 194us/sample - loss: 0.2025 - mse: 0.2025 - val_loss: 0.0594 - val_mse: 0.0594\n",
      "Epoch 2/1000\n",
      "6400/7248 [=========================>....] - ETA: 0s - loss: 0.0651 - mse: 0.0651\n",
      "Epoch 00002: val_mse improved from 0.05945 to 0.02393, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 75us/sample - loss: 0.0623 - mse: 0.0623 - val_loss: 0.0239 - val_mse: 0.0239\n",
      "Epoch 3/1000\n",
      "6400/7248 [=========================>....] - ETA: 0s - loss: 0.0320 - mse: 0.0320\n",
      "Epoch 00003: val_mse improved from 0.02393 to 0.01432, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 74us/sample - loss: 0.0308 - mse: 0.0308 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 4/1000\n",
      "6336/7248 [=========================>....] - ETA: 0s - loss: 0.0193 - mse: 0.0193\n",
      "Epoch 00004: val_mse improved from 0.01432 to 0.01061, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 76us/sample - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 5/1000\n",
      "7232/7248 [============================>.] - ETA: 0s - loss: 0.0129 - mse: 0.0129\n",
      "Epoch 00005: val_mse improved from 0.01061 to 0.00927, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 75us/sample - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0093 - val_mse: 0.0093\n",
      "Epoch 6/1000\n",
      "7200/7248 [============================>.] - ETA: 0s - loss: 0.0109 - mse: 0.0109\n",
      "Epoch 00006: val_mse improved from 0.00927 to 0.00877, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 75us/sample - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0088 - val_mse: 0.0088\n",
      "Epoch 7/1000\n",
      "6400/7248 [=========================>....] - ETA: 0s - loss: 0.0098 - mse: 0.0098\n",
      "Epoch 00007: val_mse improved from 0.00877 to 0.00855, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 74us/sample - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 8/1000\n",
      "6432/7248 [=========================>....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\n",
      "Epoch 00008: val_mse improved from 0.00855 to 0.00835, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 76us/sample - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0084 - val_mse: 0.0084\n",
      "Epoch 9/1000\n",
      "6368/7248 [=========================>....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\n",
      "Epoch 00009: val_mse improved from 0.00835 to 0.00827, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 76us/sample - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0083 - val_mse: 0.0083\n",
      "Epoch 10/1000\n",
      "7104/7248 [============================>.] - ETA: 0s - loss: 0.0086 - mse: 0.0086\n",
      "Epoch 00010: val_mse improved from 0.00827 to 0.00809, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 76us/sample - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 11/1000\n",
      "6464/7248 [=========================>....] - ETA: 0s - loss: 0.0081 - mse: 0.0081\n",
      "Epoch 00011: val_mse improved from 0.00809 to 0.00790, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 74us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0079 - val_mse: 0.0079\n",
      "Epoch 12/1000\n",
      "6368/7248 [=========================>....] - ETA: 0s - loss: 0.0081 - mse: 0.0081\n",
      "Epoch 00012: val_mse improved from 0.00790 to 0.00776, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 75us/sample - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 13/1000\n",
      "6880/7248 [===========================>..] - ETA: 0s - loss: 0.0082 - mse: 0.0082\n",
      "Epoch 00013: val_mse improved from 0.00776 to 0.00775, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 79us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 14/1000\n",
      "7008/7248 [============================>.] - ETA: 0s - loss: 0.0079 - mse: 0.0079\n",
      "Epoch 00014: val_mse improved from 0.00775 to 0.00770, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 77us/sample - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 15/1000\n",
      "6432/7248 [=========================>....] - ETA: 0s - loss: 0.0081 - mse: 0.0081\n",
      "Epoch 00015: val_mse did not improve from 0.00770\n",
      "7248/7248 [==============================] - 1s 71us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 16/1000\n",
      "6464/7248 [=========================>....] - ETA: 0s - loss: 0.0077 - mse: 0.0077\n",
      "Epoch 00016: val_mse improved from 0.00770 to 0.00766, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 75us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 17/1000\n",
      "7136/7248 [============================>.] - ETA: 0s - loss: 0.0078 - mse: 0.0078\n",
      "Epoch 00017: val_mse improved from 0.00766 to 0.00757, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 76us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 18/1000\n",
      "6528/7248 [==========================>...] - ETA: 0s - loss: 0.0081 - mse: 0.0081\n",
      "Epoch 00018: val_mse improved from 0.00757 to 0.00754, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 73us/sample - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 19/1000\n",
      "6496/7248 [=========================>....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 00019: val_mse did not improve from 0.00754\n",
      "7248/7248 [==============================] - 1s 70us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 20/1000\n",
      "6560/7248 [==========================>...] - ETA: 0s - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 00020: val_mse improved from 0.00754 to 0.00744, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 74us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 21/1000\n",
      "7200/7248 [============================>.] - ETA: 0s - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 00021: val_mse did not improve from 0.00744\n",
      "7248/7248 [==============================] - 1s 73us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 22/1000\n",
      "7232/7248 [============================>.] - ETA: 0s - loss: 0.0076 - mse: 0.0076\n",
      "Epoch 00022: val_mse did not improve from 0.00744\n",
      "7248/7248 [==============================] - 1s 73us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 23/1000\n",
      "7136/7248 [============================>.] - ETA: 0s - loss: 0.0076 - mse: 0.0076\n",
      "Epoch 00023: val_mse did not improve from 0.00744\n",
      "7248/7248 [==============================] - 1s 73us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 24/1000\n",
      "6432/7248 [=========================>....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 00024: val_mse did not improve from 0.00744\n",
      "7248/7248 [==============================] - 1s 72us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 25/1000\n",
      "7168/7248 [============================>.] - ETA: 0s - loss: 0.0077 - mse: 0.0077\n",
      "Epoch 00025: val_mse did not improve from 0.00744\n",
      "7248/7248 [==============================] - 1s 73us/sample - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0078 - val_mse: 0.0078\n",
      "Epoch 26/1000\n",
      "6400/7248 [=========================>....] - ETA: 0s - loss: 0.0080 - mse: 0.0080\n",
      "Epoch 00026: val_mse did not improve from 0.00744\n",
      "7248/7248 [==============================] - 1s 72us/sample - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 27/1000\n",
      "6336/7248 [=========================>....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\n",
      "Epoch 00027: val_mse did not improve from 0.00744\n",
      "7248/7248 [==============================] - 1s 72us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 28/1000\n",
      "6432/7248 [=========================>....] - ETA: 0s - loss: 0.0078 - mse: 0.0078\n",
      "Epoch 00028: val_mse did not improve from 0.00744\n",
      "7248/7248 [==============================] - 1s 72us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 29/1000\n",
      "6400/7248 [=========================>....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 00029: val_mse improved from 0.00744 to 0.00739, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 73us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 30/1000\n",
      "6368/7248 [=========================>....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 00030: val_mse did not improve from 0.00739\n",
      "7248/7248 [==============================] - 1s 72us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 31/1000\n",
      "7200/7248 [============================>.] - ETA: 0s - loss: 0.0074 - mse: 0.0074\n",
      "Epoch 00031: val_mse improved from 0.00739 to 0.00726, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 76us/sample - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 32/1000\n",
      "7200/7248 [============================>.] - ETA: 0s - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 00032: val_mse did not improve from 0.00726\n",
      "7248/7248 [==============================] - 1s 73us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 33/1000\n",
      "7232/7248 [============================>.] - ETA: 0s - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 00033: val_mse improved from 0.00726 to 0.00722, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 75us/sample - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 34/1000\n",
      "6336/7248 [=========================>....] - ETA: 0s - loss: 0.0077 - mse: 0.0077\n",
      "Epoch 00034: val_mse did not improve from 0.00722\n",
      "7248/7248 [==============================] - 1s 72us/sample - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 35/1000\n",
      "7168/7248 [============================>.] - ETA: 0s - loss: 0.0073 - mse: 0.0073\n",
      "Epoch 00035: val_mse did not improve from 0.00722\n",
      "7248/7248 [==============================] - 1s 73us/sample - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 36/1000\n",
      "6816/7248 [===========================>..] - ETA: 0s - loss: 0.0072 - mse: 0.0072\n",
      "Epoch 00036: val_mse did not improve from 0.00722\n",
      "7248/7248 [==============================] - 1s 86us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0077 - val_mse: 0.0077\n",
      "Epoch 37/1000\n",
      "7072/7248 [============================>.] - ETA: 0s - loss: 0.0072 - mse: 0.0072\n",
      "Epoch 00037: val_mse did not improve from 0.00722\n",
      "7248/7248 [==============================] - 1s 82us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 38/1000\n",
      "7232/7248 [============================>.] - ETA: 0s - loss: 0.0072 - mse: 0.0072\n",
      "Epoch 00038: val_mse did not improve from 0.00722\n",
      "7248/7248 [==============================] - 1s 75us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 39/1000\n",
      "7040/7248 [============================>.] - ETA: 0s - loss: 0.0070 - mse: 0.0070\n",
      "Epoch 00039: val_mse did not improve from 0.00722\n",
      "7248/7248 [==============================] - 1s 74us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 40/1000\n",
      "6912/7248 [===========================>..] - ETA: 0s - loss: 0.0072 - mse: 0.0072\n",
      "Epoch 00040: val_mse did not improve from 0.00722\n",
      "7248/7248 [==============================] - 1s 75us/sample - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 41/1000\n",
      "7040/7248 [============================>.] - ETA: 0s - loss: 0.0070 - mse: 0.0070\n",
      "Epoch 00041: val_mse did not improve from 0.00722\n",
      "7248/7248 [==============================] - 1s 75us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 42/1000\n",
      "6912/7248 [===========================>..] - ETA: 0s - loss: 0.0071 - mse: 0.0071\n",
      "Epoch 00042: val_mse improved from 0.00722 to 0.00722, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 77us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 43/1000\n",
      "7008/7248 [============================>.] - ETA: 0s - loss: 0.0070 - mse: 0.0070\n",
      "Epoch 00043: val_mse did not improve from 0.00722\n",
      "7248/7248 [==============================] - 1s 74us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 44/1000\n",
      "6400/7248 [=========================>....] - ETA: 0s - loss: 0.0071 - mse: 0.0071\n",
      "Epoch 00044: val_mse improved from 0.00722 to 0.00720, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 74us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 45/1000\n",
      "6336/7248 [=========================>....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\n",
      "Epoch 00045: val_mse did not improve from 0.00720\n",
      "7248/7248 [==============================] - 1s 72us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 46/1000\n",
      "7072/7248 [============================>.] - ETA: 0s - loss: 0.0069 - mse: 0.0069\n",
      "Epoch 00046: val_mse improved from 0.00720 to 0.00713, saving model to best_model.h5\n",
      "7248/7248 [==============================] - 1s 76us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0071 - val_mse: 0.0071\n",
      "Epoch 47/1000\n",
      "7040/7248 [============================>.] - ETA: 0s - loss: 0.0071 - mse: 0.0071\n",
      "Epoch 00047: val_mse did not improve from 0.00713\n",
      "7248/7248 [==============================] - 1s 74us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0075 - val_mse: 0.0075\n",
      "Epoch 48/1000\n",
      "6528/7248 [==========================>...] - ETA: 0s - loss: 0.0070 - mse: 0.0070\n",
      "Epoch 00048: val_mse did not improve from 0.00713\n",
      "7248/7248 [==============================] - 1s 80us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 49/1000\n",
      "7072/7248 [============================>.] - ETA: 0s - loss: 0.0071 - mse: 0.0071\n",
      "Epoch 00049: val_mse did not improve from 0.00713\n",
      "7248/7248 [==============================] - 1s 75us/sample - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0076 - val_mse: 0.0076\n",
      "Epoch 50/1000\n",
      "7168/7248 [============================>.] - ETA: 0s - loss: 0.0069 - mse: 0.0069\n",
      "Epoch 00050: val_mse did not improve from 0.00713\n",
      "7248/7248 [==============================] - 1s 73us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 51/1000\n",
      "6496/7248 [=========================>....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\n",
      "Epoch 00051: val_mse did not improve from 0.00713\n",
      "7248/7248 [==============================] - 1s 70us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 52/1000\n",
      "6464/7248 [=========================>....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\n",
      "Epoch 00052: val_mse did not improve from 0.00713\n",
      "7248/7248 [==============================] - 1s 71us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 53/1000\n",
      "6464/7248 [=========================>....] - ETA: 0s - loss: 0.0068 - mse: 0.0068\n",
      "Epoch 00053: val_mse did not improve from 0.00713\n",
      "7248/7248 [==============================] - 1s 71us/sample - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 54/1000\n",
      "6400/7248 [=========================>....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\n",
      "Epoch 00054: val_mse did not improve from 0.00713\n",
      "7248/7248 [==============================] - 1s 71us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 55/1000\n",
      "6464/7248 [=========================>....] - ETA: 0s - loss: 0.0069 - mse: 0.0069\n",
      "Epoch 00055: val_mse did not improve from 0.00713\n",
      "7248/7248 [==============================] - 1s 70us/sample - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 56/1000\n",
      "6368/7248 [=========================>....] - ETA: 0s - loss: 0.0069 - mse: 0.0069\n",
      "Epoch 00056: val_mse did not improve from 0.00713\n",
      "7248/7248 [==============================] - 1s 72us/sample - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0073 - val_mse: 0.0073\n",
      "Epoch 00056: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fee686bb048>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_nn, y_train_nn, epochs = 1000, validation_data = (X_test_nn, y_test_nn), callbacks = [es, mc],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = tf.keras.models.load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2417/2417 - 0s - loss: 0.0071 - mse: 0.0071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.007133160234519685, 0.0071331593]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model.evaluate(X_test_nn, y_test_nn, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48880127],\n",
       "       [0.48880127],\n",
       "       [0.48880127],\n",
       "       ...,\n",
       "       [0.54803896],\n",
       "       [0.54184115],\n",
       "       [0.48880127]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model.predict(X_test_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006495825545635415"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, ( clf_lit.predict(X_test) + clf_cat.predict(X_test_cat) + saved_model.predict(X_test_nn).ravel() ) / 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding neural network does not seem to improve accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try Support Vector Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007702836956989377"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svr = SVR(kernel = \"linear\", C = 0.002)\n",
    "clf_svr.fit(X_train_nn, y_train_nn)\n",
    "mean_squared_error(y_test_nn, clf_svr.predict(X_test_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008410247796822276"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svr = SVR(kernel = \"rbf\", C = 0.1)\n",
    "clf_svr.fit(X_train_nn, y_train_nn)\n",
    "mean_squared_error(y_test_nn, clf_svr.predict(X_test_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_nn, y_train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006877525258544001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test_nn, lr.predict(X_test_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006846409458613856"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr = Lasso(alpha = 0.0007)\n",
    "rr.fit(X_train_nn, y_train_nn)\n",
    "mean_squared_error(y_test_nn, rr.predict(X_test_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006858180407403158"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Ridge(alpha=0.1)\n",
    "ss.fit(X_train_nn, y_train_nn)\n",
    "mean_squared_error(y_test_nn, ss.predict(X_test_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006548083424637212"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, ( clf_lit.predict(X_test) + clf_cat.predict(X_test_cat) + saved_model.predict(X_test_nn).ravel() + ss.predict(X_test_nn) \\\n",
    "                           + rr.predict(X_test_nn)) / 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, seems that our validation scores have worsened! <br>\n",
    "So, we will stick with our ensemble of LightGBM + CatBoost<br>\n",
    "Prediction for the test data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_excel(\"/kaggle/input/flipr-hackathon-dataset/Test_dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the test data\n",
    "series_id = test_data[\"people_ID\"]\n",
    "test_data = test_data.drop([\"Name\", \"Designation\", \"Region\", \"people_ID\"],axis = 1)\n",
    "test_data = test_data.loc[~test_data.iloc[:,[0,1,3,4,7,10,11,]].isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = clf_cat.predict(test_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((pred1 < 0) | (pred1 > 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the probablility values are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "for i in reversed(cat_features):\n",
    "    le = le_list.pop()\n",
    "    test_data.iloc[:,i] = le.transform(test_data.iloc[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = clf_lit.predict(test_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vals = pd.DataFrame((pred1+pred2)/2, index = series_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vals.to_csv(\"/kaggle/working/problem1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the models for use in 2nd question:\n",
    "pickle.dump(clf_cat,open(\"cat_boost_model\",\"wb\"))\n",
    "pickle.dump(clf_lit,open(\"lightgbm_boost_model\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
